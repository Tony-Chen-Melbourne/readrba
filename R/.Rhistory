# Reading the html
html <- xml2::read_html(url) %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
dplyr::as_tibble() %>%
dplyr::filter(str_detect(value,"pdf"))   # PDFs are our only friend
page_links <- paste0("https://rba.gov.au",html$value) %>%
unique()
# The SMP is always going to be the first link
smp_link <- dplyr::tibble(link = page_links[1])
return(smp_link)
}
# Applying the function to get the links
url_list <- map_dfr(smp_page_links, smp_link_fn)
smp_mainpage <- "https://www.rba.gov.au/publications/smp/"
# Reading the html
html <- xml2::read_html(smp_mainpage) %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
dplyr::as_tibble()
# SMP page stubs
smp_page_stubs <- html %>%
dplyr::filter(str_detect(value, "/smp/\\d\\d\\d\\d/\\w"),
!stringr::str_detect(value, "box")) %>%
unique()
# Creating the links
smp_page_links <- paste0("https://www.rba.gov.au", smp_page_stubs$value)
smp_link_fn <- function(url) {
# Reading the html
html <- xml2::read_html(url) %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
dplyr::as_tibble() %>%
dplyr::filter(str_detect(value,"pdf"))   # PDFs are our only friend
page_links <- paste0("https://rba.gov.au",html$value) %>%
unique()
# The SMP is always going to be the first link
smp_link <- dplyr::tibble(link = page_links[1])
return(smp_link)
}
# Applying the function to get the links
url_list <- sapply(smp_page_links, smp_link_fn)
Q
Q
Q
# Applying the function to get the links
url_list <- sapply(smp_page_links, smp_link_fn)
url_list
url_list %>% unlist() %>% as_tibble()
url_list <- url_list %>% unlist() %>% as_tibble()
scrape_fn <- function(url) {
pdf <- pdftools::pdf_text(url)
# Clean text function
pdf_text <- pdf %>%
tm::removePunctuation() %>%
tolower() %>%
tm::stripWhitespace()
pdf_df <- pdf_text %>% dplyr::as_tibble() %>%
tidytext::unnest_tokens(word, value)
# Getting the date of the SMP
date <- url %>% stringr::str_sub(start = -11L, end = -5L) %>% paste0("-01")
date <- date %>% lubridate::as_date(format = "%Y-%m-%d")
ret <- pdf_df %>%
mutate(date = date)
return(ret)
}
ret <- map_dfr(url_list$value, scrape_fn)
scrape_fn <- function(url) {
pdf <- pdftools::pdf_text(url)
# Clean text function
pdf_text <- pdf %>%
tm::removePunctuation() %>%
tolower() %>%
tm::stripWhitespace()
pdf_df <- pdf_text %>% dplyr::as_tibble() %>%
tidytext::unnest_tokens(word, value)
# Getting the date of the SMP
date <- url %>% stringr::str_sub(start = -11L, end = -5L) %>% paste0("-01")
date <- date %>% lubridate::as_date(format = "%Y-%m-%d")
ret <- pdf_df %>%
mutate(date = date)
return(ret)
}
url <- url_list$value[20]
url
pdf <- pdftools::pdf_text(url)
# Clean text function
pdf_text <- pdf %>%
tm::removePunctuation() %>%
tolower() %>%
tm::stripWhitespace()
pdf_df <- pdf_text %>% dplyr::as_tibble() %>%
tidytext::unnest_tokens(word, value)
pdf_df
# Getting the date of the SMP
date <- url %>% stringr::str_sub(start = -11L, end = -5L) %>% paste0("-01")
date
date <- date %>% lubridate::as_date(format = "%Y-%m-%d")
date
url
pdf_df
url
View(url_list)
url_list
?print
print(url_list, max.levels = 100)
url_list$value[1]
url_list$value[50]
url_list$value[59]
pdf_text
url <- url_list$value[2]
url
pdf <- pdftools::pdf_text(url)
# Clean text function
pdf_text <- pdf %>%
tm::removePunctuation() %>%
tolower() %>%
tm::stripWhitespace()
pdf_df <- pdf_text %>% dplyr::as_tibble() %>%
tidytext::unnest_tokens(word, value)
pdf_text
pdf_text[[1]]
pdf_text[[2]]
pdf_df
write_csv("url_list.csv")
write_csv(url_list,"url_list.csv")
url <- url_list$value[50]
month <- url %>% stringr::str_remove("https://rba.gov.au/publications/smp/")
month
url <- url %>% stringr::str_remove("https://rba.gov.au/publications/smp/")
url_stub <- url
url_stub
year <- url_stub %>% stringr::str_sub(start = 1, end = 4)
year
month <- url_stub %>% stringr::str_sub(start = 6, end = 8)
month
date <- paste("1","-",month,"-","year")
date
date <- paste(year, month, "1")
date
date <- paste(year, month, "1") %>% lubridate::ymd()
date
pdf <- pdftools::pdf_text(url)
# Clean text function
pdf_text <- pdf %>%
tm::removePunctuation() %>%
tolower() %>%
tm::stripWhitespace()
pdf_df <- pdf_text %>% dplyr::as_tibble() %>%
tidytext::unnest_tokens(word, value)
# Getting the date of the SMP
date <- url %>% stringr::str_sub(start = -11L, end = -5L) %>% paste0("-01")
date <- date %>% lubridate::as_date(format = "%Y-%m-%d")
if(date %>% is.na()){
url_stub <- url %>% stringr::str_remove("https://rba.gov.au/publications/smp/")
year <- url_stub %>% stringr::str_sub(start = 1, end = 4)
month <- url_stub %>% stringr::str_sub(start = 6, end = 8)
date <- paste(year, month, "1") %>% lubridate::ymd()
}
ret <- pdf_df %>%
mutate(date = date)
View(ret)
url <- url_list$value[2]
pdf <- pdftools::pdf_text(url)
# Clean text function
pdf_text <- pdf %>%
tm::removePunctuation() %>%
tolower() %>%
tm::stripWhitespace()
pdf_df <- pdf_text %>% dplyr::as_tibble() %>%
tidytext::unnest_tokens(word, value)
# Getting the date of the SMP
date <- url %>% stringr::str_sub(start = -11L, end = -5L) %>% paste0("-01")
date <- date %>% lubridate::as_date(format = "%Y-%m-%d")
if(date %>% is.na()){
url_stub <- url %>% stringr::str_remove("https://rba.gov.au/publications/smp/")
year <- url_stub %>% stringr::str_sub(start = 1, end = 4)
month <- url_stub %>% stringr::str_sub(start = 6, end = 8)
date <- paste(year, month, "1") %>% lubridate::ymd()
}
ret <- pdf_df %>%
mutate(date = date)
View(ret)
link_fn <- function(year) {
# Creating the link
url <- stringr::str_c("https://www.rba.gov.au/monetary-policy/rba-board-minutes/",
year, "/")
# Reading the html
html <- xml2::read_html(url) %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
dplyr::as_tibble()
# Creating the individual link for statements
stub <- stringr::str_c("/monetary-policy/rba-board-minutes/",
year, "/")
html <- html %>%
dplyr::filter(stringr::str_detect(value, stub)) %>%
dplyr::filter(value != stub) %>% # Cutting out the stub choice for the year
dplyr::filter(!stringr::str_detect(value,"pdf")) # PDFs are not our friend here
ret <- tibble::tibble(url = str_c("https://rba.gov.au",html$value))
return(ret)
}
# Applying the function to get the links
today <- Sys.Date() %>% lubridate::year()
library(rvest)
library(tidyverse)
library(lubridate)
library(tm)
library(tidytext)
link_fn <- function(year) {
# Creating the link
url <- stringr::str_c("https://www.rba.gov.au/monetary-policy/rba-board-minutes/",
year, "/")
# Reading the html
html <- xml2::read_html(url) %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
dplyr::as_tibble()
# Creating the individual link for statements
stub <- stringr::str_c("/monetary-policy/rba-board-minutes/",
year, "/")
html <- html %>%
dplyr::filter(stringr::str_detect(value, stub)) %>%
dplyr::filter(value != stub) %>% # Cutting out the stub choice for the year
dplyr::filter(!stringr::str_detect(value,"pdf")) # PDFs are not our friend here
ret <- tibble::tibble(url = str_c("https://rba.gov.au",html$value))
return(ret)
}
# Applying the function to get the links
today <- Sys.Date() %>% lubridate::year()
years <- seq(2006, today, by =1) %>% as.character()
url_list <- lapply(years, link_fn) %>% unlist() %>% as_tibble()
write_csv(url_list, "url_list.csv")
url <- url_list$value[5]
html <- xml2::read_html(url)
speech_text <- html %>%
rvest::html_nodes("p") %>%
rvest::html_text()
# Get the date and location
date <- speech_text[2] %>% stringr::str_split(" ", simplify = TRUE)
location <- date[,1] %>% tm::stripWhitespace()
date <- paste(date[,3], date[,4], date[,5])
date <- date %>% lubridate::as_date(format = "%d %B %Y")
date
location
url <- url_list$value[150]
html <- xml2::read_html(url)
speech_text <- html %>%
rvest::html_nodes("p") %>%
rvest::html_text()
# Get the date and location
date <- speech_text[2] %>% stringr::str_split(" ", simplify = TRUE)
location <- date[,1] %>% tm::stripWhitespace()
date <- paste(date[,3], date[,4], date[,5])
date <- date %>% lubridate::as_date(format = "%d %B %Y")
date
date
date <- speech_text[2] %>% stringr::str_split(" ", simplify = TRUE)
location <- date[,1] %>% tm::stripWhitespace()
date
paste(date[,3], date[,4], date[,5])
location
# Get the date and location
date <- speech_text[2] %>% stringr::str_split(" ", simplify = TRUE)
length(date)
date[,2]
date[,2:num]
num <- length(date)
date[,2:num]
date <- paste(date[,2:num])
date
paste(date[,2:num])
date <- date %>% lubridate::as_date(format = "%d %B %Y")
date
# Get the date and location
date <- speech_text[2] %>% stringr::str_split(" ", simplify = TRUE)
num <- length(date)
paste(date[,2:num])
date[,2:num]
date[,2:num]
val <- date[,2:num]
date[,2:num] %>% str_c()
date[,2:num] %>% cbind()
c(date[,2:num])
paste(c(date[,2:num]))
2:num
date[1,2:num]
date %>% class()
date
date %>% as_tibble()
date %>% as_data_frame()
date %>% as_tibble()
# Get the date and location
date <- speech_text[2] %>% stringr::str_split(" ", simplify = TRUE)
date <- date %>% as_tibble()
date
location <- date[,1] %>% tm::stripWhitespace()
location <- date[,1]
location
location <- date[1,1] %>% tm::stripWhitespace()
location <- date[1,1]
location
# Get the date
url_stub <- url %>% stringr::str_remove("https://rba.gov.au/monetary-policy/rba-board-minutes/")
url_stub
date <- url_stub %>% stringr::str_sub(start = -15L, end = L)
date <- url_stub %>% stringr::str_sub(start = -15L, end = 100)
date
date <- url_stub %>% stringr::str_sub(start = -15L, end = -5L)
date
date <- url_stub %>% stringr::str_sub(start = -15L, end = -6L)
date
date <- url_stub %>% stringr::str_sub(start = -15L, end = -6L) %>% lubridate::ymd()
url <- url_list$value[2]
# Get the date
url_stub <- url %>% stringr::str_remove("https://rba.gov.au/monetary-policy/rba-board-minutes/")
url_stub
date <- url_stub %>% stringr::str_sub(start = -13L, end = -6L)
date
date <- url_stub %>% stringr::str_sub(start = -13L, end = -6L) %>%
lubridate::as_date(format = "%d%m%Y")
date
# Writing a function to purely scrape the url_list
scrape_fn <- function(url) {
html <- xml2::read_html(url)
speech_text <- html %>%
rvest::html_nodes("p") %>%
rvest::html_text()
# Get the location
location <- speech_text[2] %>% stringr::str_split(" ", simplify = TRUE)
location <- date[,1] %>% tm::stripWhitespace()
# Get the date
url_stub <- url %>% stringr::str_remove("https://rba.gov.au/monetary-policy/rba-board-minutes/")
date <- url_stub %>% stringr::str_sub(start = -15L, end = -6L) %>% lubridate::ymd()
if(date %>% is.na()) {
date <- url_stub %>% stringr::str_sub(start = -13L, end = -6L) %>%
lubridate::as_date(format = "%d%m%Y")
}
# Clean text function
speech_text <- speech_text %>%
tm::removePunctuation() %>%
tolower() %>%
tm::stripWhitespace()
speech_df <- speech_text %>% dplyr::as_tibble()
# Dropping the first row, which is html nonsense
speech_df <- speech_df[-1,]
# Un-nesting the tokens into individual words
speech_df <- speech_df %>%
tidytext::unnest_tokens(word, value)
# Adding in the date and location
speech_df <- speech_df %>%
dplyr::mutate(location = location,
date = date)
return(speech_df)
}
ret <- purrr::map_dfr(head(url_list$value), scrape_fn)
url_list$value
url
html <- xml2::read_html(url)
speech_text <- html %>%
rvest::html_nodes("p") %>%
rvest::html_text()
# Get the location
location <- speech_text[2] %>% stringr::str_split(" ", simplify = TRUE)
# Writing a function to purely scrape the url_list
scrape_fn <- function(url) {
html <- xml2::read_html(url)
speech_text <- html %>%
rvest::html_nodes("p") %>%
rvest::html_text()
# Get the location
location <- speech_text[2] %>% stringr::str_split(" ", simplify = TRUE)
location <- location[,1] %>% tm::stripWhitespace()
# Get the date
url_stub <- url %>% stringr::str_remove("https://rba.gov.au/monetary-policy/rba-board-minutes/")
date <- url_stub %>% stringr::str_sub(start = -15L, end = -6L) %>% lubridate::ymd()
if(date %>% is.na()) {
date <- url_stub %>% stringr::str_sub(start = -13L, end = -6L) %>%
lubridate::as_date(format = "%d%m%Y")
}
# Clean text function
speech_text <- speech_text %>%
tm::removePunctuation() %>%
tolower() %>%
tm::stripWhitespace()
speech_df <- speech_text %>% dplyr::as_tibble()
# Dropping the first row, which is html nonsense
speech_df <- speech_df[-1,]
# Un-nesting the tokens into individual words
speech_df <- speech_df %>%
tidytext::unnest_tokens(word, value)
# Adding in the date and location
speech_df <- speech_df %>%
dplyr::mutate(location = location,
date = date)
return(speech_df)
}
ret <- purrr::map_dfr(head(url_list$value), scrape_fn)
ret <- lapply(head(url_list$value), scrape_fn)
html <- xml2::read_html(url)
speech_text <- html %>%
rvest::html_nodes("p") %>%
rvest::html_text()
# Get the location
location <- speech_text[2] %>% stringr::str_split(" ", simplify = TRUE)
location <- location[,1] %>% tm::stripWhitespace()
# Get the date
url_stub <- url %>% stringr::str_remove("https://rba.gov.au/monetary-policy/rba-board-minutes/")
date <- url_stub %>% stringr::str_sub(start = -15L, end = -6L) %>% lubridate::ymd()
date
# Get the date
url_stub <- url %>% stringr::str_remove("https://rba.gov.au/monetary-policy/rba-board-minutes/")
date <- url_stub %>% stringr::str_sub(start = -15L, end = -6L) %>%
lubridate::as_date(format = "%Y-%m-%d")
date
if(date %>% is.na()) {
date <- url_stub %>% stringr::str_sub(start = -13L, end = -6L) %>%
lubridate::as_date(format = "%d%m%Y")
}
# Clean text function
speech_text <- speech_text %>%
tm::removePunctuation() %>%
tolower() %>%
tm::stripWhitespace()
speech_df <- speech_text %>% dplyr::as_tibble()
# Dropping the first row, which is html nonsense
speech_df <- speech_df[-1,]
# Un-nesting the tokens into individual words
speech_df <- speech_df %>%
tidytext::unnest_tokens(word, value)
# Adding in the date and location
speech_df <- speech_df %>%
dplyr::mutate(location = location,
date = date)
speech_df
# Writing a function to purely scrape the url_list
scrape_fn <- function(url) {
html <- xml2::read_html(url)
speech_text <- html %>%
rvest::html_nodes("p") %>%
rvest::html_text()
# Get the location
location <- speech_text[2] %>% stringr::str_split(" ", simplify = TRUE)
location <- location[,1] %>% tm::stripWhitespace()
# Get the date
url_stub <- url %>% stringr::str_remove("https://rba.gov.au/monetary-policy/rba-board-minutes/")
date <- url_stub %>% stringr::str_sub(start = -15L, end = -6L) %>%
lubridate::as_date(format = "%Y-%m-%d")
if(date %>% is.na()) {
date <- url_stub %>% stringr::str_sub(start = -13L, end = -6L) %>%
lubridate::as_date(format = "%d%m%Y")
}
# Clean text function
speech_text <- speech_text %>%
tm::removePunctuation() %>%
tolower() %>%
tm::stripWhitespace()
speech_df <- speech_text %>% dplyr::as_tibble()
# Dropping the first row, which is html nonsense
speech_df <- speech_df[-1,]
# Un-nesting the tokens into individual words
speech_df <- speech_df %>%
tidytext::unnest_tokens(word, value)
# Adding in the date and location
speech_df <- speech_df %>%
dplyr::mutate(location = location,
date = date)
return(speech_df)
}
ret <- sapply(head(url_list$value), scrape_fn)
ret
View(ret)
ret[[1]]
ret <- purrr::map_dfr(head(url_list$value), scrape_fn)
ret <- lapply(head(url_list$value), scrape_fn)
ret %>% as_tibble()
ret
ret %>% unlist()
ret
ret %>% as_tibble()
ret %>% rbind()
ret %>% rbind() %>%  as_tibble()
test <- ret %>% rbind() %>%  as_tibble()
test <- ret %>% cbind() %>%  as_tibble()
test <- ret %>% cbind() %>%  as_tibble()
test
ret
ret <- lapply(head(url_list$value), scrape_fn)
ret
test <- ret %>% unnest_longer()
View(ret)
ret[[1]]
ret[[1]] %>% rbind(ret[[2]])
ret %>% rbind()
source('C:/Users/Jacqueline Zhu/Google Drive/R Data work/Packages/readrba/R/get_minutes.R', echo=TRUE)
ret
ret[[]] %>% rbind()
ret[] %>% rbind()
ret[[1:4]]
ret[[1:2]]
ret[[1:3]]
?lapply
ret <- lapply(head(url_list$value), scrape_fn, simplify = TRUE)
ret <- sapply(head(url_list$value), scrape_fn, simplify = TRUE)
View(ret)
ret
ret %>% as_tibble()
ret <- purrr::map_dfr(head(url_list$value), scrape_fn)
ret <- lapply(head(url_list$value), scrape_fn)
ret <- lapply(head(url_list$value), scrape_fn)
test <- ret %>% bind_rows()
test
