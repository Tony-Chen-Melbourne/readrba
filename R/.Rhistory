speech_df <- speech_df[-1,]
# Un-nesting the tokens into individual words
speech_df <- speech_df %>%
tidytext::unnest_tokens(word, value)
# Adding in the date and location
speech_df <- speech_df %>%
dplyr::mutate(location = location,
date = date)
return(speech_df)
}
ret <- lapply(head(test$value), scrape_fn)
ret %>% unlist() %>% as_tibble()
ret %>% as_tibble()
ret
ret %>% rbind()
ret %>% reduce(left_join)
test <- ret %>% reduce(left_join)
View(test)
test <- ret %>% unnest()
ret <- purrr::map_dfr(head(test$value), scrape_fn)
url_list <- url_list %>% unlist() %>% as_tibble()
ret <- purrr::map_dfr(head(url_list$value), scrape_fn)
fsr_mainpage <- "https://www.rba.gov.au/publications/fsr/"
# Reading the html
html <- xml2::read_html(fsr_mainpage) %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
dplyr::as_tibble()
# FSR page stubs
fsr_page_stubs <- html %>%
dplyr::filter(str_detect(value, "/publications/fsr/"),
!str_detect(value, "box|article"),
nchar(value) > 23) %>%
unique()
# Creating the links
fsr_page_links <- stringr::str_c("https://www.rba.gov.au", fsr_page_stubs$value)
fsr_link_fn <- function(url) {
# Reading the html
html <- xml2::read_html(url) %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
as_tibble()
html <- html %>%
dplyr::filter(str_detect(value,"pdf"))   # PDFs are our only friend
page_links <- stringr::str_c("https://rba.gov.au",html$value) %>%
unique()
# The FSR is always going to be the first link
fsr_link <- tibble(url = page_links[1])
return(fsr_link)
}
# Applying the function to get the links
url_list <- lapply(fsr_page_links, fsr_link_fn) %>% unlist() %>%  as_tibble()
scrape_fn <- function(url) {
pdf <- pdftools::pdf_text(url)
# Clean text function
pdf_text <- pdf %>%
tm::removePunctuation() %>%
tolower() %>%
tm::stripWhitespace()
pdf_df <- pdf_text %>% dplyr::as_tibble() %>%
tidytext::unnest_tokens(word, value)
# Getting the date of the FSR
date <- pdf_text[1] %>% stringr::str_remove("financial stability review ")
date <- stringr::str_c("1 ",date )
date <- date %>% lubridate::as_date(format = "%d %b %Y")
ret <- pdf_df %>% dplyr::mutate(date = date)
return(ret)
}
ret <- purrr::map_dfr(head(url_list$value), scrape_fn)
smp_mainpage <- "https://www.rba.gov.au/publications/smp/"
# Reading the html
html <- xml2::read_html(smp_mainpage) %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
dplyr::as_tibble()
library(tidyverse)
# Reading the html
html <- xml2::read_html(smp_mainpage) %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
dplyr::as_tibble()
html
View(html)
# SMP page stubs
smp_page_stubs <- html %>%
filter(str_detect(value, "/smp/\\d\\d\\d\\d/\\w"),
!str_detect(value, "box")) %>%
unique()
smp_page_stubs
# Creating the links
smp_page_links <- str_c("https://www.rba.gov.au", smp_page_stubs$value)
smp_page_links
smp_link_fn <- function(url) {
# Reading the html
html <- read_html(url) %>%
html_nodes("a") %>%
html_attr("href") %>%
as_tibble() %>%
filter(str_detect(value,"pdf"))   # PDFs are our only friend
page_links <- str_c("https://rba.gov.au",html$value) %>%
unique()
# The FSR is always going to be the first link
smp_link <- tibble(link = page_links[1])
return(smp_link)
}
# Applying the function to get the links
url_list <- map_dfr(smp_page_links, smp_link_fn)
# Creating the links
smp_page_links <- paste0("https://www.rba.gov.au", smp_page_stubs$value)
smp_page_links
smp_link_fn <- function(url) {
# Reading the html
html <- xml2::read_html(url) %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
dplyr::as_tibble() %>%
dplyr::filter(str_detect(value,"pdf"))   # PDFs are our only friend
page_links <- paste0("https://rba.gov.au",html$value) %>%
unique()
# The SMP is always going to be the first link
smp_link <- dplyr::tibble(link = page_links[1])
return(smp_link)
}
# Applying the function to get the links
url_list <- map_dfr(smp_page_links, smp_link_fn)
View(url_list)
??removePunctuation
??unnest_tokens
?str_remove
url <- url_list$link[1]
pdf <- pdftools::pdf_text(url)
# Clean text function
pdf_text <- pdf %>%
tm::removePunctuation() %>%
tolower() %>%
tidytext::stripWhitespace()
pdf_df <- pdf_text %>% dplyr::as_tibble() %>%
tidytext::unnest_tokens(word, value)
# Getting the date of the SMP
prefix <- "https://rba.gov.au/publications/smp/"
?stripWhitespace
# Clean text function
pdf_text <- pdf %>%
tm::removePunctuation() %>%
tolower() %>%
tm::stripWhitespace()
pdf_df <- pdf_text %>% dplyr::as_tibble() %>%
tidytext::unnest_tokens(word, value)
pdf_df
# Getting the date of the SMP
prefix <- "https://rba.gov.au/publications/smp/"
suffix <- str_remove(url, prefix)
suffix
pdf_text
url
date <- url %>% str_sub(start = -11L, end = -4L)
date
date <- url %>% str_sub(start = -11L, end = -5L)
date
# Getting the date of the SMP
date <- url %>% str_sub(start = -11L, end = -5L) %>% paste0("-01")
date
date <- date %>% lubridate::as_date(format = "%Y-%m-%d")
date
install.packages("parsedate")
main_stub <- "https://www.rba.gov.au/speeches/2000/"
today <- Sys.Date()
years <- seq(1990, today %>% lubridate::year(), by = 1)
years
library(tidyverse)
years <- seq(1990, today %>% lubridate::year(), by = 1)
main_url <- paste0(today, years,"/")
main_url
main_stub <- "https://www.rba.gov.au/speeches/"
main_url <- paste0(today, years,"/")
main_url
main_url <- paste0(main_stub, years,"/")
main_url
url <- main_url[1]
html <- xml2::read_html(url) %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
dplyr::as_tibble()
View(html)
url <- main_url[5]
url
html <- xml2::read_html(url) %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
dplyr::as_tibble()
html <- xml2::read_html(url) %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
dplyr::as_tibble() %>%
dplyr::filter(str_detect(value,"pdf"))   # PDFs are our only friend
html <- xml2::read_html(url) %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
dplyr::as_tibble()
library(tidyverse)
main_stub <- "https://www.rba.gov.au/speeches/"
today <- Sys.Date()
years <- seq(1990, today %>% lubridate::year(), by = 1)
main_url <- paste0(main_stub, years,"/")
main_url
# Creating the links
link_fn <- function(url) {
html <- xml2::read_html(url) %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
dplyr::as_tibble() %>%
dplyr::filter(str_detect(value,"pdf"))   # PDFs are our only friend
}
html <- read_html(speech_main_page) %>%
html_nodes("a") %>%
html_attr("href") %>%
as_tibble()
url <- main_url[2]
html <- xml2::read_html(url) %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
dplyr::as_tibble() %>%
dplyr::filter(str_detect(value,"pdf"))   # PDFs are our only friend
View(html)
html <- xml2::read_html(url) %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
dplyr::as_tibble()
url
html <- xml2::read_html(url) %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
dplyr::as_tibble() %>%
dplyr::filter(str_detect(value,"pdf"))   # PDFs are our only friend
html
url <- main_url[15]
html <- xml2::read_html(url) %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
dplyr::as_tibble() %>%
dplyr::filter(str_detect(value,"pdf"))   # PDFs are our only friend
url
html <- xml2::read_html(url) %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
dplyr::as_tibble() %>%
dplyr::filter(str_detect(value,"html"))   # PDFs are our only friend
html <- xml2::read_html(url) %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
dplyr::as_tibble() %>%
dplyr::filter(stringr::str_detect(value,"html")) %>%   # HTMLs are our only friend
dplyr::filter(stringr::str_detect(value,"speech")) %>%   # HTMLs are our only friend
dplyr::filter(!stringr::str_detect(value,"list"))   # HTMLs are our only friend
ret <- paste0("https://www.rba.gov.au/",html)
ret
html
ret <- paste0("https://www.rba.gov.au/",html$value)
ret
# Creating the links
link_fn <- function(url) {
html <- xml2::read_html(url) %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
dplyr::as_tibble() %>%
dplyr::filter(stringr::str_detect(value,"html")) %>%   # HTMLs are our only friend
dplyr::filter(stringr::str_detect(value,"speech")) %>%   # HTMLs are our only friend
dplyr::filter(!stringr::str_detect(value,"list"))   # HTMLs are our only friend
ret <- paste0("https://www.rba.gov.au/",html$value)
return(ret)
}
main_url
url_links <- lapply(main_url, link_fn) %>% unlist() %>% as_tible()
url_links <- lapply(main_url, link_fn) %>% unlist() %>% as_tibble()
View(url_links)
url <- url_links$value[450]
html <- xml2::read_html(url)
speech_text <- html %>%
rvest::html_nodes("p") %>%
rvest::html_text()
speech_text
# Get the speaker and position
date <- speech_text[2] %>% tm::stripWhitespace()
date
# Get the speaker and position
information <- speech_text[2] %>% tm::stripWhitespace()
# Get the speaker and position
information <- speech_text[2] %>%
tm::stripWhitespace() %>%
stringr::str_split(" ", simplify = TRUE)
information
information <- speech_text[2] %>%
tm::stripWhitespace() %>%
tm::stripWhitespace() %>%
stringr::str_split(" ", simplify = TRUE)
information
information <- speech_text[2] %>%
tm::removePunctuation() %>%
tm::stripWhitespace()
information <- speech_text[2] %>%
tm::removePunctuation() %>%
tm::stripWhitespace() %>%
stringr::str_split(" ", simplify = TRUE)
information
url <- url_links$value[120]
speech_text <- html %>%
rvest::html_nodes("p") %>%
rvest::html_text()
information <- speech_text[2] %>%
tm::removePunctuation() %>%
tm::stripWhitespace() %>%
stringr::str_split(" ", simplify = TRUE)
information
html <- xml2::read_html(url)
speech_text <- html %>%
rvest::html_nodes("p") %>%
rvest::html_text()
# Get the speaker and position
information <- speech_text[2] %>%
tm::removePunctuation() %>%
tm::stripWhitespace() %>%
stringr::str_split(" ", simplify = TRUE)
information
information <- speech_text[2]
information
information <- speech_text[2] %>%
tm::removePunctuation()
information
information <- speech_text[2] %>%
tm::removePunctuation() %>%
tm::stripWhitespace()
information
information <- speech_text[2] %>%
stringr::str_split("\n", simplify = TRUE)
information
information <- speech_text[2] %>%
stringr::str_split("\n", simplify = TRUE) %>%
tm::removePunctuation() %>%
tm::stripWhitespace()
information
information <- speech_text[2] %>%
stringr::str_split("\n", simplify = TRUE) %>%
tm::removePunctuation()
information
information <- speech_text[2] %>%
stringr::str_split("\n", simplify = TRUE)
information
# Get the speaker and position
information <- speech_text[2] %>%
stringr::str_split("\n", simplify = TRUE) %>%
speaker <- information[,3]
speaker
speaker <- information[,3]
speaker
information <- speech_text[2] %>%
stringr::str_split("\n", simplify = TRUE) %>%
# Get the speaker and position
information <- speech_text[2] %>%
stringr::str_split("\n", simplify = TRUE)
information <- speech_text[2] %>%
stringr::str_split("\n", simplify = TRUE)
information
speaker <- information[,2]
speaker
speaker <- information[,2] %>% tm::stripWhitespace()
speaker
speaker <- information[,2] %>% tm::stripWhitespace() %>% trimws()
speaker
position <- information[,3] %>% tm::stripWhitespace() %>% trimws()
position
speech_text <- speech_text %>%
tm::removePunctuation() %>%
tolower() %>%
tm::stripWhitespace()
speech_text
speech_text <- speech_text %>%
tm::removePunctuation() %>%
tolower() %>%
tm::stripWhitespace()
speech_text
speech_df <- speech_text %>% dplyr::as_tibble()
speech_df
speech_df <- speech_text %>% dplyr::as_tibble()
speech_df
# Clean text function
speech_text <- speech_text %>%
tm::removePunctuation() %>%
tolower() %>%
tm::stripWhitespace()
speech_text
# Dropping the first row, which is html nonsense
speech_df <- speech_df[-1,]
speech_df <- speech_text %>% dplyr::as_tibble()
speech_df
# Dropping the first four rows, which are throat clearing
speech_df <- speech_df[-1:4,]
# Dropping the first four rows, which are throat clearing
speech_df <- speech_df[-(1:4),]
speech_df
# Un-nesting the tokens into individual words
speech_df <- speech_df %>%
tidytext::unnest_tokens(word, value)
speech_df
# Adding in the date and location
speech_df <- speech_df %>%
dplyr::mutate(speaker = speaker,
position = position)
speech_df
View(url_links)
html <- xml2::read_html(url)
speech_text <- html %>%
rvest::html_nodes("p") %>%
rvest::html_text()
speech_text
# Get the date
date <- speech_text[3] %>%
stringr::str_split("\n", simplify = TRUE)
date
date <- date[,5]
date
date <- date[,4]
# Get the date
date <- speech_text[3] %>%
stringr::str_split("\n", simplify = TRUE)
date <- date[,4]
date
# Get the date
date <- speech_text[3] %>%
stringr::str_split("\n", simplify = TRUE)
date <- date[,4] %>% tm::stripWhitespace() %>% trimws()
date
date <- date[,4] %>% tm::stripWhitespace() %>% trimws() %>%
lubridate::as_date(format = "%d %B %Y")
# Get the date
date <- speech_text[3] %>%
stringr::str_split("\n", simplify = TRUE)
date <- date[,4] %>% tm::stripWhitespace() %>% trimws() %>%
lubridate::as_date(format = "%d %B %Y")
date
# Clean text function
speech_text <- speech_text %>%
tm::removePunctuation() %>%
tolower() %>%
tm::stripWhitespace()
speech_df <- speech_text %>% dplyr::as_tibble()
#' @import stringr
#' @import dplyr
#' @import xml2
#' @import tibble
#' @import purrr
#' @import lubridate
#' @import pdftools
#' @import tm
#' @import tidytext
#' @export
read_fsr <- function() {
### Creating URLs ---------------------------------------
fsr_mainpage <- "https://www.rba.gov.au/publications/fsr/"
# Reading the html
html <- xml2::read_html(fsr_mainpage) %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
dplyr::as_tibble()
# FSR page stubs
fsr_page_stubs <- html %>%
dplyr::filter(str_detect(value, "/publications/fsr/"),
!str_detect(value, "box|article"),
nchar(value) > 23) %>%
unique()
# Creating the links
fsr_page_links <- stringr::str_c("https://www.rba.gov.au", fsr_page_stubs$value)
fsr_link_fn <- function(url) {
# Reading the html
html <- xml2::read_html(url) %>%
rvest::html_nodes("a") %>%
rvest::html_attr("href") %>%
as_tibble()
html <- html %>%
dplyr::filter(str_detect(value,"pdf"))   # PDFs are our only friend
page_links <- stringr::str_c("https://rba.gov.au",html$value) %>%
unique()
# The FSR is always going to be the first link
fsr_link <- tibble(url = page_links[1])
return(fsr_link)
}
# Applying the function to get the links
url_list <- lapply(fsr_page_links, fsr_link_fn) %>% unlist() %>%  as_tibble()
# Scraping the FSR text
scrape_fn <- function(url) {
pdf <- pdftools::pdf_text(url)
# Clean text function
pdf_text <- pdf %>%
tm::removePunctuation() %>%
tolower() %>%
tm::stripWhitespace()
pdf_df <- pdf_text %>% dplyr::as_tibble() %>%
tidytext::unnest_tokens(word, value)
# Getting the date of the FSR
date <- pdf_text[1] %>% stringr::str_remove("financial stability review ")
date <- stringr::str_c("1 ",date )
date <- date %>% lubridate::as_date(format = "%d %b %Y")
ret <- pdf_df %>% dplyr::mutate(date = date)
return(ret)
}
ret <- purrr::map_dfr(head(url_list$value), scrape_fn)
return(ret)
}
fsr_data <- readrba::read_fsr()
View(fsr_data)
fsr_data$date %>% unique()
library(tidyverse)
fsr_data$date %>% unique()
